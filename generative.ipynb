{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://vene.ro/figurative-comparisons/#dataset\n",
    "\n",
    "http://www.ep.liu.se/ecp/126/004/ecp16126004.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.aclweb.org/anthology/P13-3013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ansible.uk/misc/striking.html\n",
    "\n",
    "in similes.txt, all phrases starting with 'Like' and 'As' were removed\n",
    "\n",
    "Any old English, like \"a flutter'd bird\" were translated to modern spellings.\n",
    "\n",
    "https://github.com/allenai/bilm-tf\n",
    "\n",
    "http://www.english.su.se/polopoly_fs/1.100637.1347450592!/menu/standard/file/SMF_2012_Patrick_Hanks_plenary.pdf\n",
    "\n",
    "https://www.academicscope.com/literature-review-on-automatic-simile-and-metaphor-recognizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McLachlan, G. J., & Rathnayake, S. (2014). On the number of components in a Gaussian mixture model. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 4(5), 341-355.\n",
    "\n",
    "http://aclweb.org/anthology/W18-09\n",
    "\n",
    "Computational considerations of comparisons and similes\n",
    "https://www.aclweb.org/anthology/P13-3013\n",
    "\n",
    "A Computational System of Metaphor Generation with Evaluation Mechanism\n",
    "https://rd.springer.com/content/pdf/10.1007%2F978-3-642-15822-3_18.pdf\n",
    "\n",
    "Towards the Automatic Mining of Similes in Literary Texts\n",
    "http://www.ep.liu.se/ecp/126/004/ecp16126004.pdf\n",
    "\n",
    "Literature Review on Automatic Simile and Metaphor Recognizers\n",
    "https://www.academicscope.com/literature-review-on-automatic-simile-and-metaphor-recognizers/\n",
    "\n",
    "https://github.com/alexWhitworth/emclustr/blob/master/R/clust_mvn_miss.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import permutations\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simile data\n",
    "\n",
    "Components are manually extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_ixs = [0,1,2,3]\n",
    "component_tags = ['topic', 'event', 'ground', 'vehicle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_json('similes.json', orient='records')\n",
    "\n",
    "%time sentences['nlp'] = sentences['original'].apply(lambda text: nlp(text))\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('google-news.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(tokens):\n",
    "    \n",
    "#     elmo_v = elmo.embed_sentence(tokens)[1]\n",
    "\n",
    "    w2v_v = np.ones((len(tokens), 300)) * np.nan\n",
    "    for i, tk in enumerate(tokens):\n",
    "        try:\n",
    "            w2v_v[i, :] = model[tk]\n",
    "        except:\n",
    "            if tk not in OOV:\n",
    "                OOV.append(tk)\n",
    "    \n",
    "    return w2v_v #np.hstack([elmo_v, w2v_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OOV = []\n",
    "\n",
    "# topic_vectors =   np.zeros((len(sentences), 300))\n",
    "topic_vectors =   np.random.normal(scale=0.2, size=(len(sentences), 300))\n",
    "# event_vectors =   np.zeros((len(sentences), 300))\n",
    "event_vectors =   np.random.normal(scale=0.2, size=(len(sentences), 300))\n",
    "# ground_vectors =  np.zeros((len(sentences), 300))\n",
    "ground_vectors =  np.random.normal(scale=0.2, size=(len(sentences), 300))\n",
    "# vehicle_vectors = np.zeros((len(sentences), 300))\n",
    "vehicle_vectors = np.random.normal(scale=0.2, size=(len(sentences), 300))\n",
    "\n",
    "for ix, s in sentences.iterrows():\n",
    "    \n",
    "    tokens = np.array([tk.text for tk in s['nlp']])\n",
    "    embedding = embed_tokens(tokens)\n",
    "        \n",
    "    for tag, vec in zip(component_tags, [topic_vectors, event_vectors, ground_vectors, vehicle_vectors]):\n",
    "        if s[tag]:\n",
    "            emb = embedding[s[tag], :]\n",
    "            \n",
    "            if np.isnan(emb).all():\n",
    "                print(ix, tag)\n",
    "                mn = np.random.normal(scale=1, size=(300,))\n",
    "            else:\n",
    "                mn = np.nanmean(emb, axis=0)\n",
    "\n",
    "            vec[ix] = mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity with Latent Component\n",
    "\n",
    "If we remove a component, the overall syntatical expression remains with a predictability that hopefully can be leveraged with a Gaussian Mixture Model (or some other latent variable maximization algorithm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = {'original': 'Their love burned as passionate as', 'topic': [0,1], 'ground': [4], 'event': [2], 'vehicle': None}\n",
    "\n",
    "tokens = np.array([tk.text for tk in nlp(inp['original'])])\n",
    "\n",
    "example_topic_vector =   np.zeros((1, 300)) #.random.normal(scale=0.5, size=(1, 300))\n",
    "example_event_vector =   np.zeros((1, 300)) #.random.normal(scale=0.5, size=(1, 300))\n",
    "example_ground_vector =  np.zeros((1, 300)) #.random.normal(scale=0.5, size=(1, 300))\n",
    "example_vehicle_vector = np.zeros((1, 300)) #.random.normal(scale=0.5, size=(1, 300))\n",
    "\n",
    "embedding = embed_tokens(tokens)\n",
    "\n",
    "if inp['topic']:\n",
    "    example_topic_vector = embedding[inp['topic']].mean(axis=0)\n",
    "if inp['event']:\n",
    "    example_event_vector = embedding[inp['event']].mean(axis=0)\n",
    "if inp['ground']:\n",
    "    example_ground_vector = embedding[inp['ground']].mean(axis=0)\n",
    "if inp['vehicle']:\n",
    "    example_vehicle_vector = embedding[inp['vehicle']].mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yax_labs = ['', '', '', '']\n",
    "xax_labs = ['', '', '', '']\n",
    "ds = np.zeros((4,4))\n",
    "\n",
    "comp = 14\n",
    "\n",
    "norm = [topic_vectors[comp, :], event_vectors[comp, :], ground_vectors[comp, :], vehicle_vectors[comp, :]]\n",
    "example = [example_topic_vector, example_event_vector, example_ground_vector, example_vehicle_vector]\n",
    "labels = ['topic', 'event', 'ground', 'vehicle']\n",
    "\n",
    "for i in range(4):\n",
    "    e = sentences.loc[comp, [labels[i], 'original']].values\n",
    "    phrase1 = np.array([tk.text for tk in nlp(e[1])])[e[0]]\n",
    "    yax_labs[i] = ' '.join(phrase1)\n",
    "    if inp[labels[i]]:\n",
    "        d = 1 - pdist(np.vstack([norm[i], example[i]]), 'cosine')\n",
    "    \n",
    "        ds[i, i] = d\n",
    "        \n",
    "        e = sentences.loc[v2, [labels[i], 'original']].values\n",
    "        \n",
    "        phrase2 = np.array([tk.text for tk in nlp(inp['original'])])[inp[labels[i]]]\n",
    "        xax_labs[i] = ' '.join(phrase2)\n",
    "    elif inp[labels[i]] is None:\n",
    "        xax_labs[i] = 'LAT'\n",
    "\n",
    "g = sns.heatmap(ds, vmin=0, vmax=1, annot=True, cmap='copper')\n",
    "g.set_xticklabels(xax_labs, rotation=30);\n",
    "g.set_yticklabels(yax_labs, rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_component(simile_ix, comp):\n",
    "    comp = sentences.loc[simile_ix, comp]\n",
    "    return ' '.join([tk.text for i, tk in enumerate(sentences.loc[simile_ix, 'nlp']) if i in comp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_comps = 50\n",
    "dec = PCA(n_components=pca_comps)\n",
    "decf = dec.fit_transform(ground_vectors)\n",
    "    \n",
    "plt.plot(list(range(pca_comps)), np.cumsum(dec.explained_variance_ratio_))\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.grid(alpha=0.25)\n",
    "plt.ylim([0,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_length = 4\n",
    "\n",
    "decomp_topic = PCA(n_components=decomp_length)\n",
    "decomp_event = PCA(n_components=decomp_length)\n",
    "decomp_ground = PCA(n_components=decomp_length)\n",
    "decomp_vehicle = PCA(n_components=decomp_length)\n",
    "\n",
    "redu_array = np.array([topic_vectors, event_vectors, ground_vectors, vehicle_vectors])\n",
    "\n",
    "redu_topic_vectors = decomp_topic.fit_transform(redu_array[0])\n",
    "redu_event_vectors = decomp_event.fit_transform(redu_array[1])\n",
    "redu_ground_vectors = decomp_ground.fit_transform(redu_array[2])\n",
    "redu_vehicle_vectors = decomp_vehicle.fit_transform(redu_array[3])\n",
    "\n",
    "redu_all = np.hstack([redu_topic_vectors, redu_event_vectors, redu_ground_vectors, redu_vehicle_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redu_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ns = [1]\n",
    "\n",
    "growf = 1.2\n",
    "\n",
    "while ns[-1] * growf < len(redu_all):\n",
    "    \n",
    "    ns.append(np.ceil(ns[-1] * growf).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# of components relies on the complexity of the input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "m, n = redu_all.shape\n",
    "\n",
    "# calculate some metric for GMM of n components\n",
    "for n_comp in ns:\n",
    "    gm = GaussianMixture(n_components=n_comp, covariance_type='full', n_init=10, tol=1e-4)\n",
    "    gmf = gm.fit(redu_all)\n",
    "    \n",
    "#     P = n_comp * (n + (n*(n+1))/2)\n",
    "\n",
    "#     s = np.sum(gmf.score_samples(redu_all))\n",
    "#     score = -1*s + 0.5 * P * np.log(m)\n",
    "    \n",
    "    scores.append(gmf.aic(redu_all))\n",
    "    \n",
    "plt.subplots(figsize=(16,5))\n",
    "plt.ylabel('AIC')\n",
    "plt.xlabel('# of Components')\n",
    "plt.grid(alpha=0.2)\n",
    "g = sns.barplot(ns, scores, color='r')\n",
    "\n",
    "g.patches[np.argmin(scores)].set_color('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 21.2 ms, total: 1.03 s\n",
      "Wall time: 607 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100,\n",
       "        means_init=None, n_components=58, n_init=10, precisions_init=None,\n",
       "        random_state=None, reg_covar=1e-06, tol=0.0001, verbose=0,\n",
       "        verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_comps = ns[np.argmin(scores)]\n",
    "gm = GaussianMixture(n_components=n_comps, covariance_type='full', n_init=10, tol=1e-4)\n",
    "\n",
    "%time gmf = gm.fit(redu_all)\n",
    "\n",
    "gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gmf.predict(redu_all)\n",
    "\n",
    "np.unique(p, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.loc[p == 3, 'original']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_latent_data(input_dict):\n",
    "    global component_ixs\n",
    "\n",
    "    tv = np.zeros((1, 300)) #.random.normal(scale=0.5, size=(1, 300))\n",
    "    ev = np.zeros((1, 300)) #.random.normal(scale=0.5, size=(1, 300))\n",
    "    gv = np.zeros((1, 300)) #.random.normal(scale=0.5, size=(1, 300))\n",
    "    vv = np.zeros((1, 300)) #.random.normal(scale=0.5, size=(1, 300))\n",
    "\n",
    "    embedding = embed_tokens(input_dict['tokens'])\n",
    "    \n",
    "    latent = []\n",
    "\n",
    "    if inp['topic']:\n",
    "        tv = embedding[inp['topic']].mean(axis=0)\n",
    "    else:\n",
    "        latent.append('topic')\n",
    "\n",
    "    if inp['event']:\n",
    "        ev = embedding[inp['event']].mean(axis=0)\n",
    "    else:\n",
    "        latent.append('event')\n",
    "\n",
    "    if inp['ground']:\n",
    "        gv = embedding[inp['ground']].mean(axis=0)\n",
    "    else:\n",
    "        latent.append('ground')\n",
    "\n",
    "    if inp['vehicle']:\n",
    "        vv = embedding[inp['vehicle']].mean(axis=0)\n",
    "    else:\n",
    "        latent.append('vehicle')   \n",
    "        \n",
    "    out = []\n",
    "    \n",
    "    for mix in permutations(latent):\n",
    "        combined = np.array([tv, ev, gv, vv])\n",
    "        mixout = []\n",
    "        for lat in mix:\n",
    "            ix = component_tags.index(lat)\n",
    "            del component_ixs[ix]\n",
    "            latent, vocab_ix, comp, post_prob = cycle_potential_vocab(lat, combined[component_ixs])\n",
    "            mixout.append([ix, vocab_ix, comp, post_prob])\n",
    "            combined[ix] = redu_array[ix, vocab_ix]\n",
    "            component_ixs = [0,1,2,3]\n",
    "            \n",
    "        out.append(mixout)\n",
    "        \n",
    "    return np.array(out)\n",
    "\n",
    "def cycle_potential_vocab(latent_component, remaining_components):\n",
    "    '''\n",
    "    Given an unknown vector, but three known vectors, iterate over all vocabularly possibilities to\n",
    "    maximize the posterior.\n",
    "    \n",
    "    i.e. choose the best vocab for the missing data\n",
    "    '''\n",
    "    if latent_component is 'topic':\n",
    "        posteriors = np.zeros((len(topic_vectors), 2))\n",
    "        for i, vec in enumerate(topic_vectors):\n",
    "            posteriors[i, :] = choose_best_mixture(gmf, vec, remaining_components[0], remaining_components[1], remaining_components[2])\n",
    "    \n",
    "    elif latent_component is 'event':\n",
    "        posteriors = np.zeros((len(event_vectors), 2))\n",
    "        for i, vec in enumerate(event_vectors):\n",
    "            posteriors[i, :] = choose_best_mixture(gmf, remaining_components[0], vec, remaining_components[1], remaining_components[2])\n",
    "    \n",
    "    elif latent_component is 'ground':\n",
    "        posteriors = np.zeros((len(ground_vectors), 2))\n",
    "        for i, vec in enumerate(ground_vectors):\n",
    "            posteriors[i, :] = choose_best_mixture(gmf, remaining_components[0], remaining_components[1], vec, remaining_components[2])\n",
    "    \n",
    "    elif latent_component is 'vehicle':\n",
    "        posteriors = np.zeros((len(vehicle_vectors), 2))\n",
    "        for i, vec in enumerate(vehicle_vectors):\n",
    "            posteriors[i, :] = choose_best_mixture(gmf, remaining_components[0], remaining_components[1], remaining_components[2], vec)\n",
    "    \n",
    "    m = np.argmax(posteriors[:, 1])\n",
    "    return latent_component, m, posteriors[m, 0], posteriors[m, 1]\n",
    "    \n",
    "def choose_best_mixture(gmm, topic, event, ground, vehicle):\n",
    "    '''\n",
    "    Given four vector components, return the MAP estimate of which Gaussian component the vectors should\n",
    "    belong to.\n",
    "    \n",
    "    i.e. choose the best model for and probability of the given data\n",
    "    '''\n",
    "    redu_topic = decomp_topic.transform(np.atleast_2d(topic))\n",
    "    redu_event = decomp_event.transform(np.atleast_2d(event))\n",
    "    redu_ground = decomp_ground.transform(np.atleast_2d(ground))\n",
    "    redu_vehicle = decomp_vehicle.transform(np.atleast_2d(vehicle))\n",
    "    \n",
    "    redu_all = np.hstack([redu_topic, redu_event, redu_ground, redu_vehicle])\n",
    "    \n",
    "    posteriors = np.zeros(n_comps)\n",
    "    \n",
    "    for gmm_comp in range(n_comps):\n",
    "        mean = gmm.means_[gmm_comp, :]\n",
    "        cov = gmm.covariances_[gmm_comp, :, :]\n",
    "        \n",
    "        rv = multivariate_normal(mean=mean, cov=cov)\n",
    "                \n",
    "        posteriors[gmm_comp] = rv.logpdf(redu_all) \n",
    "                        \n",
    "    return (np.argmax(posteriors), np.max(posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piece_fit(fit):\n",
    "    complete = {}\n",
    "    for comp_ix in component_ixs:\n",
    "        tag = component_tags[comp_ix]\n",
    "\n",
    "        if comp_ix in fit[:, 0].astype(int):\n",
    "            w = np.where(fit[:, 0].astype(int) == comp_ix)[0]\n",
    "            text = get_component(int(fit[w, 1]), tag)\n",
    "            complete[tag] = text\n",
    "        else:\n",
    "            complete[tag] = \" \".join(inp[\"tokens\"][inp[tag]])\n",
    "\n",
    "    return complete\n",
    "\n",
    "def format_simile(components):\n",
    "    t = components['topic']\n",
    "    e = components['event']\n",
    "    g = components['ground']\n",
    "    v = components['vehicle']\n",
    "    if g and t and e and v:\n",
    "        return f'{t} {e} as {g} as {v}'\n",
    "    elif not g and t and e and v:\n",
    "        return f'{t} {e} like {v}'\n",
    "    elif not t and g and not e and v:\n",
    "        return f'{g} like {v}'\n",
    "    elif not e and t and g and v:\n",
    "        return f'{t} as {g} as {v}'\n",
    "    else:\n",
    "        return components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My mind clatters like a wave of the wild sea'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = {'original': 'My mind', 'topic': [0,1], 'ground': None, 'event': None, 'vehicle': None}\n",
    "inp['tokens'] = np.array([tk.text for tk in nlp(inp['original'])])\n",
    "\n",
    "z = cycle_latent_data(inp)\n",
    "\n",
    "best_fit = np.argmax(np.sum(z[:, :, 3], axis=1))\n",
    "\n",
    "format_simile(piece_fit(z[best_fit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
